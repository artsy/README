---
title: Kubernetes Upgrade
description: All steps of a blue-green switch to an upgraded Kubernetes cluster
---

# The Cutover to the new Cluster in Production

## Migration
All of the steps below are to be done mostly in a repo called [substance](https://github.com/artsy/substance). For more context look at this [PR](https://github.com/artsy/substance/pull/279).
---
# prepare binaries
- backup binaries locally (in case they get overwritten)
```shell
cp /usr/local/bin/kubectl /usr/local/bin/kubectl.bak
cp /usr/local/bin/kops /usr/local/bin/kops.bak
cp /usr/local/bin/aws-iam-authenticator /usr/local/bin/aws-iam-authenticator.bak
```
- update binaries, if necessary
```shell
./manage.py install
```
check the versions
```shell
kops version # should be 1.19.3
kubectl version # should be 1.19.16
aws-iam-authenticator version # should be 0.4.0
```
  
- update terraform to v0.12.31, if necessary
    https://github.com/artsy/infrastructure/blob/main/bin/setup

# spin up cluster
- create cluster using v1.19 k8s base image already in use in staging
   ```shell
    ./manage.py create_cluster --environment production --role federation --name <...new name...> --ami-name ami-037a49d5d844bca2c
    ```
    when prompted, edit new cluster's spec.yml to match each instance group's min/max node count with existing prod cluster's with the exception of setting min to 1 for foreground/background groups
- fetch master kube config
  ```shell
  export KOPS_RUN_OBSOLETE_VERSION=true
  export KOPS_STATE_STORE=s3://artsy-kops-state-store
  export KOPS_CLUSTER_NAME=kubernetes-production-<..new cluster name..>.artsy.systems
  rm  ~/.kube/config
  kops export kubecfg --admin
  ```
  `~/.kube/config` now has master config for <..new cluster name..>
- update master kube config
  - download old master kube config from 1pass
  - add new cluster's master config to it
  - upload new file to 1pass
- delete coredns autoscaler that kops added at cluster creation
  ```shell
  kubectl config use-contet kubernetes-production-<..new cluster name..>.artsy.systems
  kubectl -n kube-system delete deployment coredns-autoscaler
  kubectl -n kube-system delete configmap coredns-autoscaler
  kubectl -n kube-system delete sa coredns-autoscaler
  kubectl delete clusterroles coredns-autoscaler
  kubectl delete clusterrolebinding coredns-autoscaler
  ```
- check in a master node the expiration date of all the certs generated by kops
  ```
  find /mnt/ -type f -name *.crt -print -exec openssl x509 -enddate -noout -in {} \;
  find /etc/kubernetes/ -type f -name *.crt -print -exec openssl x509 -enddate -noout -in {} \;
  ```
  If there are certs expiring in one year, cut a ticket to find out how they will be renewed.

# apply cluster services from old prod cluster

1. In the [services directory](https://github.com/artsy/substance/tree/main/clusters/production/federation/kubernetes-production-pictor.artsy.systems/services) of the old production cluster go through the following yml files: 
  - ci-role
  - dev-role
  - storarge-class
  - data-application
  - data-application
  
  and copy over configmaps from old prod cluster.
  ```shell
  kubectl --context production -n data-application get configmap
  ```
  In the following yml files
  - limits
  - kubernetes-state-metrics
  - metrics-server
  - k8s-authenticator-config
  
  edit cluster name. 
  And in the following list of yml files
  - coredns
  - nodelocaldns
  - cluster-autoscaler
  
  edit the cluster name and edit foreground/background min node count to 1.

2. NLBs (Network Load Balancers?)
    - add cluster to vpc subnets as a sharer like in this [example](https://github.com/artsy/infrastructure/blob/dd0508ef4a051d00100231bd436000ac800723d6/terraform/production/vpc.tf#L72) or in this [PR](https://github.com/artsy/infrastructure/pull/410):
    ```yml
        tags = {
            "Name" = "Private production 1b"
            "kubernetes.io/cluster/kubernetes-production-pictor.artsy.systems" = "shared" <---
            "kubernetes.io/role/internal-elb" = "1"
        }
    ```
    - ingress-nginx-namespace-tls secret
        copy from old prod
        ```shell
        kubectl --context kubernetes-production-<...old cluster name...>.artsy.systems -n ingress-nginx get secret ingress-nginx-namespace-tls -o yaml
        ```
    - artsy-systems-tls
        copy from old prod
        ```shell
        kubectl --context kubernetes-production-<...old cluster name...>.artsy.systems get secret artsy-systems-tls -o yaml
        ```
    - artsy-tls-2022
    ```shell
        kubectl --context kubernetes-production-<...old cluster name...>.artsy.systems get secret artsy-tls-2022 -o yaml
    ```
    - nginx-ingress-base
    - nginx-ingress-template
    - nginx-ingress-deployment-internal
    - nginx-ingress-external-alt
    - nginx-ingress-external

3. dashboard
4. dashboard-ingress
    change hostname to `-blue`
5. elasticsearch.yml
    change ingress hostname to `-blue`
6. install elasticsearch license
7. rabbitmq-secrets
    copy from old prod
    ```
    kubectl --context kubernetes-production-<...old cluster name...>.artsy.systems get secret rabbitmq-secrets -o yaml
    ```
8. rabbitmq.yml
    change ingress hostname to `-blue`
    flip `RABBITMQ_CLUSTER_IS_RUNNING` to `0`
    once applied flip it back to `1`
9. papertrail-config configmap
    copy from old prod:
    ```shell
    kubectl --context kubernetes-production-<...old cluster name...>.artsy.systems -n kube-system get configmap papertrail-config -o yaml
    ```
10. logspout
11. datadog-config configmap
    copy from old prod:
    ```
    kubectl --context kubernetes-production-<...old cluster name...>.artsy.systems get configmap datadog-config -o yaml
    ```
    change the Tag in the configmap to `-blue`
12. datadog-confd configmap
    copy from old prod:
    ```
    kubectl --context kubernetes-production-<...old cluster name...>.artsy.systems get configmap datadog-confd -o yaml
    ```
13. datadog

# create new ci/dev/admin kube configs
- download configs from `s3://artsy-citadel/k8s/`
- add new cluster name as `'prod-blue'`
- upload new files back to s3

# Add Infrastructure repo's terraform remote state to main.tf  

https://github.com/artsy/substance/commit/49848dfedc3f4b56b36d6d3110201343102bf147

# setup dns for vpc-internal services

1. copy from old prod's dns.tf
    edit:
    - internal ingress controller
        `nginx-blue.prd.artsy.systems -> <..internal NLB dns..>`
        find out the NLB dns via:
        ```
        kubectl -n ingress-nginx get service ingress-nginx-controller-internal
        ```
    - internal DNS for ES and Rabbit related services
        
        alias them to 
        - nginx-blue.artsy.systems
        - cerebro-blue.prd.artsy.systems
        - elasticsearch-blue.prd.artsy.systems
        - kibana-blue.prd.artsy.systems
        - kubernetes-blue.prd.artsy.systems
        - rabbitmq-management-blue.prd.artsy.systems
        - rabbitmq-blue.prd.artsy.systems
2. terrafom apply

# validate hokusai/ci on new cluster
1. validate hokusai commands on new cluster
  - create a test config-dev that associates 'production' context with new cluster
    ```shell
      cd ~/.kube
      cp config config.bak
      rm config
      cp config-dev config-dev-draco
    ```
    ...edit config-dev-draco...
    ```
    ln -s config-dev-draco config
    ```
  - create hokusai-sandbox deployment in new cluster
    ```
    hokusai production create
    ```
  - run a bunch of hokusai production commands
    ```shell 
    hokusai production refresh
    hokusai production env set foo=bar
    hokusai production env get
    hokusai production logs
    hokusai production run --tty sh
    hokusai production update --skip-checks
    hokusai production deploy <...find an image tag via: hokusai registry images...>
    hokusai production delete
    ```

2. validate ci on new cluster
  - publish a dev hokusai orb that configures hokusai using config-ci-<...new cluster name...>.yml
      https://github.com/artsy/orbs/blob/dba8d2834e53d46714481787c49bb945cfe0364a/src/hokusai/hokusai.yml#L53
  - create config-ci-<...new cluster name...>.yml with this content:
    ```
    kubectl-version: 1.19.16
    kubectl-config-file: s3://artsy-citadel/k8s/config-ci-<...new cluster name...>
    ```
    
    upload to s3://artsy-provisioning-public/hokusai/config-ci-<...new cluster name...>.yml

  - create s3://artsy-citadel/k8s/config-ci-<...new cluster name...>
    
    - same content as s3://artsy-citadel/k8s/config-ci
    - but have 'production' context' be associated with new cluster
    - upload tot s3://artsy-citadel/k8s/config-ci-<...new cluster name...>

  - update hokusai-sandbox circleci config to use the dev hokusai orb
  - create deployment again
    ```
    hokusai production create
    ```
  - trigger a prod deploy for hokusai-sandbox
  - hokusai production status
    - confirm that pods are refreshed by ci deploy

# test <...new cluster name...> internet NLB (Network Load Balancer)

with config-dev-<...new cluster name...> still in place

- remove hokusai-sandbox ingress IP restriction
    - edit hokusai/production.yml.j2
        ```
        hokusai production update --skip-checks
        ```
- curl against <...new cluster name...>'s internet NLB

    example: 
    ```
    curl --header "host: hokusai-sandbox.artsy.net" a56d67b501c094843af4ed96414d90fe-5423274d93f6e7c1.elb.us-east-1.amazonaws.com
    ```

# setup gravity ES dual-indexing

For Details read this [Notion Doc](https://www.notion.so/artsy/Reindexing-Elasticsearch-68d6bbd94b924783872bea437d60c313).

- launch gravity-sidekiq-es-secondary deployment
- create indexes in new ES
    - double check list of all existent indexes
    ```shell
    curl -s elasticsearch.prd.artsy.systems/_cat/indices
    update the notion doc
    ```
    - create those indexes in new ES
- enable `ELASTICSEARCH_SECONDARY_ENABLED` in old gravity deployment
- queue full indexing jobs for each index

# index Positron

Find details in this [Notion Doc](https://www.notion.so/artsy/Reindexing-ElasticSearch-68d6bbd94b924783872bea437d60c313#4adc0464b2cc4996b47250fdddc2df88)

# confirm old/new ES are in sync
1. compare doc count of each index
```shell
curl -s elasticsearch-blue.prd.artsy.systems/_cat/indices
```
2. check index settings and fields
```shell
curl http://elasticsearch-blue.prd.artsy.systems:9200/artworks_production | jq
```
3. spot check searches
```
curl -vvv 'http://elasticsearch-blue.prd.artsy.systems:9200/artworks_production/_search?q=caled'
```
4. scale down gravity-sidekiq-es-secondary deployment to 1 pod

# shortly before cutover
1. announce freeze on slack
    - no running local hokusai production commands ⚠️
    - no merging release PRs ⚠️
2. create deploy blocks in Horizon

On the console in horizon do: 
```ruby
Project.all.each{|p| p.deploy_blocks.create!(description: "k8s migration") }
```
3. raise new cluster's foreground/background node group min.
    - edit clusters/production/federation/kubernetes-production-draco.artsy.systems/services/cluster-autoscaler.yml

      foreground min = 25 (temporary for cutover)
      background min = 2
4. import rabbitmq broker definition

In this file:

s3://artsy-backup/rabbitmq/rabbitmq-management.prd.artsy.systems-broker-definitions.json

upload using the UI at:

rabbitmq-management-blue.prd.artsy.systems

5. make a fresh prod backup
```shell
./manage.py create_backup --context production
```
6. restore project specs from the fresh backup into new cluster
```shell
./manage.py list_backups --context production

./manage.py restore_backup --backup-from production --restore-to prod-blue <...backup id...>
```

when prompted:
- edit configmaps.yml to remove datadog configmaps
- skip daemonsets.yaml
- skip statefulsets
- skip replicationcontrollers
- edit services.yml, remove these:
    - cerebro-lb-internal
    - datadog
    - es
    - es-internal
    - kibana-lb-internal
    - rabbitmq
    - rabbitmq-internal
    - rmq-management
- edit ingresses.yml, remove:
    - cerebro
    - elasticsearch
    - kibana
    - rabbitmq-management
7. scale down singleton gravity sidekiq deployemnts
```shell
kubectl --context prod-blue scale --replicas=0 deployments gravity-sidekiq-bid
kubectl --context prod-blue scale --replicas=0 deployments gravity-sidekiq-causality
```
8. disable cronjobs in new cluster
```shell
for cron in `kubectl --context prod-blue get cronjobs | grep -v NAME | awk '{print $1}'`
do
kubectl --context prod-blue patch cronjobs $cron -p '{"spec" : {"suspend" : true }}'
done
```
9. apply causality hokusai specs in new cluster
this is b/c the backup doesn't capture all the resources defined for causality
10. ensure all pods are healthy
```shell
kubectl --context prod-blue get pods --all-namespaces
```
11. prepare jenkins for shutdown
https://joe.artsy.net/manage

# cutover
1. cutover traffic to internal apps
- change nginx.prd.artsy.systems
from:
a7382faf4056b4ce98c9f4d967b0a111-a0b4ba1cfa25cc03.elb.us-east-1.amazonaws.com
to:
ac58db7e5f52d44b68d09ca9e46ed7f3-946804432f14c370.elb.us-east-1.amazonaws.com

- change nginx-blue.prd.artsy.systems
from:
ac58db7e5f52d44b68d09ca9e46ed7f3-946804432f14c370.elb.us-east-1.amazonaws.com
to:
a7382faf4056b4ce98c9f4d967b0a111-a0b4ba1cfa25cc03.elb.us-east-1.amazonaws.com

so that all the non-blues point to new cluster, and the blues point to old.

2. cutover traffic to internet apps
- change nginx.artsy.net
from:
a75dde85e0b2e47a8a8811edb1d47970-fb2399d98912db21.elb.us-east-1.amazonaws.com
to:
a56d67b501c094843af4ed96414d90fe-5423274d93f6e7c1.elb.us-east-1.amazonaws.com

3. cutover cronjobs
- disable cronjobs in old cluster
```shell
for cron in `kubectl --context production get cronjobs | grep -v NAME | awk '{print $1}'`
do
    kubectl --context production patch cronjobs $cron -p '{"spec" : {"suspend" : true }}'
done
```
- enable cronjobs in new cluster
```shell
for cron in `kubectl --context prod-blue get cronjobs | grep -v NAME | awk '{print $1}'`
do
    kubectl --context prod-blue patch cronjobs $cron -p '{"spec" : {"suspend" : false }}'
done
```
4. scale up singleton gravity sidekiq deployemnts
5. update ES/rabbit/k8s-dashboard ingresses
- edit draco specs, remove -blue from ingress hostnames
- re-apply specs
- edit old cluster specs, add -blue to ingress hostnames
- re-apply specs
6. reindex Positron (for Articles)
https://www.notion.so/artsy/Reindexing-ElasticSearch-68d6bbd94b924783872bea437d60c313#4adc0464b2cc4996b47250fdddc2df88
7. migrate jenkins
With `prod-blue` pointing to old cluster, `production` pointing to new cluster:
- Prepare Jenkins for shutdown: `Manage Jenkins > Prepare for Shutdown`
- Cancel any running job
- record jenkins volume's information in old cluster
```shell
kubectl --context prod-blue get pvc jenkins-data
kubectl --context prod-blue describe pv <pv-name>
```
- scale down jenkins in old cluster
```shell
kubectl --context prod-blue get pods | grep jenkins
kubectl --context prod-blue scale --replicas=0 deployments jenkins
kubectl --context prod-blue get pods | grep jenkins
```

- take a snapshot of the volume
    volume id is in the get pv output
- set volume's reclaim policy to Retain (requires admin role) in old cluster
```shell
kubectl --context prod-blue patch pv <pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
```
- export the pvc and pv definitions from old cluster
```shell
kubectl --context prod-blue get pvc jenkins-data -o yaml > jenkins-pvc.yml
kubectl --context prod-blue get pv <pv-name> -o yaml > jenkins-pv.yml
```
- apply jenkins spec in new cluster
```shell
kubectl --context production apply -f services/jenkins.yml
```
- record pvc and pv information in new cluster
```shell
kubectl --context production get pvc jenkins-data
kubectl --context production get pv <pv-name>
kubectl --context production describe pv <pv-name>
```
- scale down jenkins in new cluster
```shell 
kubectl --context production get pods | grep jenkins
kubectl --context production scale --replicas=0 deployments jenkins
kubectl --context production get pods | grep jenkins
```
- delete the pvc in new cluster
```shell
kubectl --context production delete pvc jenkins-data
```
- confirm that pvc and pv are deleted in new cluster
```shell
kubectl --context production get pvc
kubectl --context production get pv
```
- apply old cluster's pvc spec to new cluster
    - edit the file first to remove metadata
    - keep annontations, uid, volumeName
```shell
kubectl --context production apply -f jenkins-pvc.yml
kubectl --context production get pvc jenkins-data
```
- get uid of pvc in new cluster
```shell
kubectl --context production get pvc jenkins-data -o jsonpath='{.metadata.uid}'; echo
```
- check on snapshot progress. wait for it to finish.
- apply old cluster's pv spec to new cluster associating with uid of pvc
    - edit the file first to remove metadata, but keep these annotations:
    ```shell
    kubernetes.io/createdby: aws-ebs-dynamic-provisioner
    pv.kubernetes.io/bound-by-controller: "yes"
    pv.kubernetes.io/provisioned-by: kubernetes.io/aws-ebs
    ```
    set `persistentVolumeReclaimPolicy` to `Delete`
    set `claimRef > uid` to the id from above
    ```shell
    kubectl --context production apply -f jenkins-pv.yml
    kubectl --context production get pv
    kubectl --context production get pvc
    ```
- delete pvc and pv in old cluster
    ```shell
    kubectl --context prod-blue delete pvc jenkins-data
    kubectl --context prod-blue delete pv <pv-name>
    ``` 
- edit volume's tag via ec2 console
    Set these tags to new cluster:
    - `Name`
    - `KubernetesCluster`
- scale up jenkins in new cluster
```shell
kubectl --context production get pods | grep jenkins
kubectl --context production scale --replicas=1 deployments jenkins
kubectl --context production get pods | grep jenkins
```
- verify that the pod in new cluster has volume mounted
```shell
kubectl --context production get pod | grep jenkins
kubectl --context production exec -it <name> bash
df -h
ls -ltrh /var/lib/jenkins/logs/tasks
```
- point joe.artsy.net to nginx.artsy.net
    cloudflare

# after cutover
- clear deploy blocks in Horizon
```ruby
DeployBlock.unresolved.each{|db| db.update!(resolved_at: Time.now) }
```